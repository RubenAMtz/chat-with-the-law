1. Download your cross-encoder from huggingface:  
  1.1 For example: [cross-encoder/mmarco-mMiniLMv2-L12-H384-v1](https://huggingface.co/cross-encoder/mmarco-mMiniLMv2-L12-H384-v1/tree/main)  
  1.2 Make sure to do: `git lfs install` as files are compressed, this way you can download the uncompressed version, you need it.  
  1.3 Make sure to place the downloaded folder at the same level as the `scoring` folder.  
2. Configure your scoring script.  
  2.1 Define an `init` function, here is where you load your model.
  2.2 Configure your `input` and `output` schemas. These are essentially examples of what the endpoint should expect to receive, in that sense, it will try to convert whatever it receives to the datatypes defined in your input schema. This also happens for the response.  
  2.3 Define a `run` function, here you make the predictions and return a response.
3. Get ready to deploy:  
  3.1 Make sure you setup the correct `subscription_id`, `resource_group` and `workspace`.  
  3.2 Name your endpoint with `endpoint_name`.  
  3.3 Set the model path, I usually go for the full absolute path, not sure if relative paths work.  
    3.3.1 This step creates an environment variable named `AZUREML_MODEL_DIR`, you can later retrieve it in your scoring script. Notice that this is a 'new' path/folder, the model is copied to this location, if your model needs reference to extra configuration files you can always look for the original folder by going up a few levels in the directory tree.  
  3.4 Define the conda environment file path, again, test if relative paths work if you need to.  
  3.5 Select a docker container to build your project. List of available containers can be found here: [containers](https://github.com/Azure/AzureML-Containers).
  3.6 Define your deployment name.
  3.7 Define the parent folder where your code lives, this is specially important if you want to get access to your project files other than your model.
  3.8 Define the relative path to your scoring script (relative to the code folder from the previous step).
  3.9 Select an instance type, here is a list of supported instances for Azure Endpoints: [Supported Instances](https://learn.microsoft.com/en-us/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list?view=azureml-api-2)
  3.10 Depending on the instance you select and the model you are using you might want to change the request configuration settings, i.e. incresing time-out time. [OnlineRequestSettings](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-online-endpoints?view=azureml-api-2&tabs=cli#http-status-codes)


